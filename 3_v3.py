# -*- coding: utf-8 -*-
"""3 v3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rs-uUxX7UxHJcf6SgqUfbEyGc6HFCnLE
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Para escalar el target
from sklearn.preprocessing import StandardScaler as TargetScaler

pd.set_option("display.max_columns", None)
pd.set_option("display.width", 120)

CSV_PATH = "BTCUSDT_1d_last_year.csv"

df = pd.read_csv(CSV_PATH)
df.columns = [c.lower() for c in df.columns]

ohlc = ["open","high","low","close"]
missing = [c for c in ohlc if c not in df.columns]
if missing:
    raise ValueError(f"Faltan columnas OHLC: {missing}")

# Cambios porcentuales (%)
for c in ohlc:
    df[f"{c}_pct"] = df[c].pct_change()*100.0

# Volumen opcional
vol_cols = [c for c in ["volume","volumen"] if c in df.columns]
vcol = vol_cols[0] if vol_cols else None
if vcol:
    df[f"{vcol}_pct"] = df[vcol].pct_change()*100.0

# Target: retorno % del próximo período (cierre)
df["close_pct_next"] = df["close_pct"].shift(-1)

df.head()

"""## 2) *Feature engineering* (lags, medias móviles, volatilidad)
- Lags de `close_pct` (1..5)
- `ma3`, `ma7`: medias móviles de `close_pct`
- `volatilidad_7`: desvío estándar móvil (7)

"""

# Lags
N_LAGS = 5
for k in range(1, N_LAGS+1):
    df[f"close_pct_lag{k}"] = df["close_pct"].shift(k)

# Medias móviles y volatilidad
df["ma3"] = df["close_pct"].rolling(3).mean()
df["ma7"] = df["close_pct"].rolling(7).mean()
df["volatilidad_7"] = df["close_pct"].rolling(7).std()

# Feature set base
feature_cols = ["open_pct","high_pct","low_pct","close_pct","ma3","ma7","volatilidad_7"]
if vcol:
    feature_cols.append(f"{vcol}_pct")

# + lags
feature_cols += [f"close_pct_lag{k}" for k in range(1, N_LAGS+1)]

# Drop NaN de pct_change/shift/rolling
df_model = df.dropna(subset=feature_cols + ["close_pct_next"]).copy()

X = df_model[feature_cols]
y = df_model["close_pct_next"]

X.shape, y.shape, X.head()

"""## 3) Split temporal y **escalado del *target***
Usamos el 20% final como *test* y escalamos **solo con *train***.  
Luego **desescalamos** las predicciones para evaluar en % real.

"""

n = len(df_model)
cut = int(n*0.8)

X_train, X_test = X.iloc[:cut], X.iloc[cut:]
y_train, y_test = y.iloc[:cut], y.iloc[cut:]

# Escalado del target
scaler_y = TargetScaler()
y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1,1)).ravel()
y_test_scaled = scaler_y.transform(y_test.values.reshape(-1,1)).ravel()

len(X_train), len(X_test)

"""## 4) Modelos y *grid search* con **TimeSeriesSplit**
Incluimos ahora un **RandomForestRegressor**.

"""

tscv = TimeSeriesSplit(n_splits=5)

models = {
    "LinearRegression": {
        "pipe": Pipeline([("scaler", StandardScaler()), ("model", LinearRegression())]),
        "param_grid": {}
    },
    "KNNRegressor": {
        "pipe": Pipeline([("scaler", StandardScaler()), ("model", KNeighborsRegressor())]),
        "param_grid": {
            "model__n_neighbors": [3, 5, 7, 9],
            "model__weights": ["uniform","distance"]
        }
    },
    # "DecisionTreeRegressor": {
    #     "pipe": Pipeline([("model", DecisionTreeRegressor(random_state=42))]),
    #     "param_grid": {
    #         "model__max_depth": [3, 5, 7, 10, None],
    #         "model__min_samples_leaf": [1, 2, 4]
    #     }
    # },
    "DecisionTreeRegressor": {
    "pipe": Pipeline([("model", DecisionTreeRegressor(random_state=42))]),
    "param_grid": {
        "model__max_depth": [8, 12, 20, None],
        "model__min_samples_leaf": [1, 2],
        "model__min_samples_split": [2, 4]
        }
    },
    "RandomForestRegressor": {
        "pipe": Pipeline([("model", RandomForestRegressor(random_state=42, n_jobs=-1))]),
        "param_grid": {
            "model__n_estimators": [200, 400],
            "model__max_depth": [5, 10, 15, None],
            "model__min_samples_leaf": [1, 2, 4]
        }
    }
}

results = []
best_estimators = {}

for name, cfg in models.items():
    pipe = cfg["pipe"]
    grid = cfg["param_grid"]
    if grid:
        search = GridSearchCV(
            estimator=pipe,
            param_grid=grid,
            scoring="neg_mean_absolute_error",
            cv=tscv,
            n_jobs=-1
        )
        search.fit(X_train, y_train_scaled)  # usamos target escalado
        best = search.best_estimator_
        best_estimators[name] = best
        cv_mae = -search.best_score_
        cv_params = search.best_params_
    else:
        pipe.fit(X_train, y_train_scaled)
        best = pipe
        best_estimators[name] = best
        cv_mae = None
        cv_params = {}

    # Predicción en test (escala del target)
    preds_scaled = best.predict(X_test)
    # Desescalar a %
    preds = scaler_y.inverse_transform(np.array(preds_scaled).reshape(-1,1)).ravel()

    mae = mean_absolute_error(y_test, preds)
    rmse = np.sqrt(mean_squared_error(y_test, preds))
    r2 = r2_score(y_test, preds)

    results.append({
        "modelo": name,
        "cv_mae": cv_mae,
        "mejores_params": cv_params,
        "test_mae": mae,
        "test_rmse": rmse,
        "test_r2": r2
    })

res_df = pd.DataFrame(results).sort_values("test_mae")
res_df

"""## 5) Curva Real vs Predicción del mejor modelo

"""

best_name = res_df.iloc[0]["modelo"]
best_model = best_estimators[best_name]

preds_scaled = best_model.predict(X_test)
preds = scaler_y.inverse_transform(np.array(preds_scaled).reshape(-1,1)).ravel()

plt.figure(figsize=(10,4))
plt.plot(y_test.values, label="Real")
plt.plot(preds, label=f"Predicción ({best_name})")
plt.title("Cambio % de close (siguiente período)")
plt.legend()
plt.xlabel("Observación (orden temporal)")
plt.ylabel("%")
plt.tight_layout()
plt.show()

res_df

# Aseguramos los modelos en orden
model_names = ["LinearRegression", "KNNRegressor", "DecisionTreeRegressor", "RandomForestRegressor"]

# Creamos una figura con 4 subgráficos verticales
fig, axes = plt.subplots(4, 1, figsize=(12, 12), sharex=True, sharey=True)

for i, name in enumerate(model_names):
    if name not in best_estimators:
        continue  # si algún modelo no se entrenó, lo salteamos

    model = best_estimators[name]
    preds_scaled = model.predict(X_test)
    preds = scaler_y.inverse_transform(np.array(preds_scaled).reshape(-1,1)).ravel()

    ax = axes[i]
    ax.plot(y_test.values, label="Real", linewidth=1.8)
    ax.plot(preds, label=f"Predicción ({name})", linewidth=1.5)
    ax.set_title(f"{name} – Cambio % de close (siguiente período)")
    ax.set_ylabel("%")
    ax.legend(loc="upper right")

axes[-1].set_xlabel("Observación (orden temporal)")
plt.tight_layout()
plt.show()